{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5565baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jaayu\\miniconda3\\envs\\wigner_test\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices: 1\n",
      "[]\n",
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "from dipy.io.image import load_nifti\n",
    "from dipy.align.reslice import reslice\n",
    "from scipy.ndimage import affine_transform\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "num_gpus = strategy.num_replicas_in_sync\n",
    "print(f'Number of devices: {num_gpus}')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "print(tf.config.list_logical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf69ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images in dataset:  484\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_list(dataset):\n",
    "    dataset_list = []\n",
    "    \n",
    "    if dataset == 'CC':\n",
    "        files = ['./dataset_cc359.txt']\n",
    "    elif dataset == 'NFBS':\n",
    "        files = ['./dataset.txt']\n",
    "    elif dataset == 'both':\n",
    "        files = ['./dataset.txt']\n",
    "        files.append('./dataset_cc359.txt')\n",
    "\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            lin = f.readline()\n",
    "            while(lin):\n",
    "                dataset_list.append(lin[:-1])\n",
    "                lin = f.readline()\n",
    "    print('Total Images in dataset: ', len(dataset_list))\n",
    "    return dataset_list\n",
    "\n",
    "def datasetHelperFunc(path):\n",
    "    transform_vol, mask = None, None\n",
    "    if isinstance(path, bytes):\n",
    "        path = str(path.decode('utf-8'))\n",
    "\n",
    "    if 'CC' in path:\n",
    "        vol, affine, voxsize = load_nifti(\n",
    "            '/N/project/grg_data/data/skullstripping_datasets/CC359/Original/'+path, return_voxsize=True)\n",
    "        mask, _ = load_nifti(\n",
    "            '/N/project/grg_data/data/skullstripping_datasets/CC359/STAPLE/'+path[:-7]+'_staple.nii.gz')\n",
    "        mask[mask < 1] = 0  # Values <1 in the mask is background\n",
    "        vol = vol*mask\n",
    "    else:\n",
    "        #vol, affine, voxsize = load_nifti(str(path.decode('utf-8')), return_voxsize=True)\n",
    "        vol, affine, voxsize = load_nifti(path, return_voxsize=True)\n",
    "        mask, _ = load_nifti(path[:-7]+'mask.nii.gz')\n",
    "        mask[mask < 1] = 0  # Values <1 in the mask is background\n",
    "        vol = vol*mask\n",
    "\n",
    "    if mask is not None:\n",
    "        mask, _ = transform_img(mask, affine, voxsize)\n",
    "        # Handling negative pixels, occurred as a result of preprocessing\n",
    "        mask[mask < 0] *= -1\n",
    "        mask = np.expand_dims(mask, -1)\n",
    "    transform_vol, _ = transform_img(vol, affine, voxsize)\n",
    "    # Handling negative pixels, occurred as a result of preprocessing\n",
    "    transform_vol[transform_vol < 0] *= -1\n",
    "    transform_vol = (transform_vol-np.min(transform_vol)) / \\\n",
    "        (np.max(transform_vol)-np.min(transform_vol))\n",
    "    transform_vol = np.expand_dims(transform_vol, -1)\n",
    "    return tf.convert_to_tensor(transform_vol, tf.float32), tf.convert_to_tensor(mask, tf.float32)\n",
    "\n",
    "\n",
    "dataset_list = get_dataset_list('both')\n",
    "\n",
    "bs = 48\n",
    "suffix = 'B48-both'\n",
    "test_size = len(dataset_list) - (len(dataset_list)//bs)*bs\n",
    "\n",
    "lis = dataset_list[-test_size:]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(lis)\n",
    "\n",
    "dataset = dataset.map(lambda x: tf.numpy_function(func=datasetHelperFunc, inp=[x], Tout=[tf.float32, tf.float32]),\n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = dataset.batch(bs).prefetch(tf.data.experimental.AUTOTUNE).take(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e3cf2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2b34ebe2a740>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VQVAE(\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            num_channels=(32, 64, 128),\n",
    "            num_res_channels=(32, 64, 128),\n",
    "            num_res_layers=3,\n",
    "            downsample_parameters=((2, 4, 1, 'same'), (2, 4, 1, 'same'), (2, 4, 1, 'same')),\n",
    "            upsample_parameters=((2, 4, 1, 'same', 0), (2, 4, 1, 'same', 0), (2, 4, 1, 'same')),\n",
    "            num_embeddings=256,\n",
    "            embedding_dim=32,\n",
    "            num_gpus=float(num_gpus),\n",
    "            kernel_resize=False)\n",
    "\n",
    "test_epoch = 64\n",
    "model.load_weights(os.path.join('./checkpoints-vqvae-monai-scaled-128', suffix, str(test_epoch)+'.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f410a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, _ in test_dataset:\n",
    "    np.save(f'./reconst_scaled_vqvae3d_monai/original-{suffix}.npy', x.numpy())\n",
    "    reconst = model(x)\n",
    "    loss = tf.reduce_mean((reconst-x)**2)\n",
    "    print(f'Test Loss is {loss}')\n",
    "    np.save(f'./reconst_scaled_vqvae3d_monai/reconst3d-{suffix}-epoch{test_epoch}.npy', reconst.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a68a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, _ in test_dataset:\n",
    "    print(x.numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d59ac",
   "metadata": {},
   "source": [
    "# Test BraTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "906628d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dipy.io.image import load_nifti\n",
    "from dipy.align.reslice import reslice\n",
    "from scipy.ndimage import affine_transform\n",
    "from fury.actor import slicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42a1e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol, affine, voxsize = load_nifti(r'D:\\DiPY\\SyntheticMRI\\BraTS2021_00495_t1.nii.gz', return_voxsize=True)\n",
    "mask, affine, voxsize = load_nifti(r'D:\\DiPY\\SyntheticMRI\\BraTS2021_00495_seg.nii.gz', return_voxsize=True)\n",
    "vol = vol.astype(np.float32)\n",
    "mask = mask.astype(np.float32)\n",
    "mask[mask < 1] = 0  # Values <1 in the mask is background\n",
    "vol = vol*mask # zero out the background or non-region of interest areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "436265b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_img_brats(image, affine, voxsize, final_shape = (128, 128, 128)):\n",
    "    temp_image, affine_temp = reslice(image, affine, voxsize, (2, 2, 2))\n",
    "    temp_image = slicer(temp_image, affine_temp).resliced_array()\n",
    "    print(temp_image.shape)\n",
    "    \n",
    "    current_shape = temp_image.shape\n",
    "\n",
    "    pad_x = (final_shape[0] - current_shape[0]) // 2\n",
    "    pad_y = (final_shape[1] - current_shape[1]) // 2\n",
    "    pad_z = (final_shape[2] - current_shape[2]) // 2\n",
    "\n",
    "    # Ensure the padding is equally distributed\n",
    "    pad_width = ((pad_x, pad_x), (pad_y, pad_y), (pad_z, pad_z))\n",
    "\n",
    "    transformed_img = np.pad(temp_image, pad_width, mode='constant', constant_values=0)\n",
    "    print(\"Padded image shape:\", transformed_img.shape)\n",
    "\n",
    "    return transformed_img, affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e0b1de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 120, 78)\n",
      "Padded image shape: (128, 128, 128)\n",
      "(120, 120, 78)\n",
      "Padded image shape: (128, 128, 128)\n",
      "(128, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "mask, _ = transform_img_brats(mask, affine, voxsize)\n",
    "transform_vol, _ = transform_img_brats(vol, affine, voxsize)\n",
    "\n",
    "mask[mask < 0] *= -1 # Handling negative pixels, occurred as a result of preprocessing\n",
    "mask = np.expand_dims(mask, -1)\n",
    "\n",
    "transform_vol[transform_vol < 0] *= -1 # Handling negative pixels, occurred as a result of preprocessing\n",
    "\n",
    "transform_vol = (transform_vol-np.min(transform_vol)) / \\\n",
    "    (np.max(transform_vol)-np.min(transform_vol))\n",
    "transform_vol = np.expand_dims(transform_vol, -1)\n",
    "print(transform_vol.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "496d6a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 240, 155)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba08428526d14a13b685aee00f920f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=77, description='Slice Index', max=154), Output()), _dom_classes=('widge…"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "# brain_images = transform_vol\n",
    "brain_images = vol\n",
    "print(brain_images.shape)\n",
    "def plot_brain_slices(slice_index):\n",
    "    n_images = 1\n",
    "    cols = int(np.ceil(np.sqrt(n_images)))\n",
    "    rows = int(np.ceil(n_images / cols))\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))\n",
    "\n",
    "    axes.imshow(brain_images[:, :, slice_index], cmap='gray')  # Adjust index based on your data's shape\n",
    "    axes.set_title(f'Model Epoch, Slice {slice_index}')\n",
    "    axes.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "slice_slider = widgets.IntSlider(min=0, max=brain_images.shape[2]-1, step=1, value=brain_images.shape[2]//2, description='Slice Index')\n",
    "widgets.interactive(plot_brain_slices, slice_index=slice_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ab39b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 128, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e10600bdf3400a93a95c8e22200601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=64, description='Slice Index', max=127), Output()), _dom_classes=('widge…"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "brain_images = transform_vol\n",
    "print(brain_images.shape)\n",
    "def plot_brain_slices(slice_index):\n",
    "    n_images = 1\n",
    "    cols = int(np.ceil(np.sqrt(n_images)))\n",
    "    rows = int(np.ceil(n_images / cols))\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))\n",
    "    print(brain_images[:, :, slice_index].shape)\n",
    "    axes.imshow(brain_images[:, :, slice_index], cmap='gray')  # Adjust index based on your data's shape\n",
    "    axes.set_title(f'Model Epoch, Slice {slice_index}')\n",
    "    axes.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "slice_slider = widgets.IntSlider(min=0, max=brain_images.shape[2]-1, step=1, value=brain_images.shape[2]//2, description='Slice Index')\n",
    "widgets.interactive(plot_brain_slices, slice_index=slice_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d01f7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4dadf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
