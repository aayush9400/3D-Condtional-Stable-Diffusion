{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5565baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices: 1\n",
      "[]\n",
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 11:12:52.194131: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /N/soft/rhel7/deeplearning/Python-3.10.5/libcutensor/lib/11:/N/soft/rhel7/deeplearning/Python-3.10.5/libcusparse_lt/lib64:/N/soft/rhel7/deeplearning/Python-3.10.5/cuda/lib64:/N/soft/rhel7/deeplearning/Python-3.10.5/cuda/lib:/N/soft/rhel7/deeplearning/Python-3.10.5/:/N/soft/rhel7/openmpi/intel/4.0.1/lib:/N/soft/rhel7/intel/19.5/compilers_and_libraries_2019.5.281/linux/compiler/lib/intel64:/N/soft/rhel7/intel/19.5/compilers_and_libraries_2019.5.281/linux/ipp/lib/intel64:/N/soft/rhel7/intel/19.5/compilers_and_libraries_2019.5.281/linux/compiler/lib/intel64_lin:/N/soft/rhel7/intel/19.5/compilers_and_libraries_2019.5.281/linux/mkl/lib/intel64_lin:/N/soft/rhel7/intel/19.5/compilers_and_libraries_2019.5.281/linux/tbb/lib/intel64/gcc4.7:/N/soft/rhel7/intel/19.5/debugger_2019/iga/lib:/N/soft/rhel7/intel/19.5/debugger_2019/libipt/intel64/lib:/N/soft/rhel7/intel/19.5/compilers_and_libraries_2019.5.281/linux/daal/lib/intel64_lin:/N/soft/rhel7/gcc/12.1.0/lib64:/N/soft/rhel7/gcc/12.1.0/lib:/N/soft/rhel7/gcc/infrastructure/lib\n",
      "2023-10-16 11:12:52.194258: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-10-16 11:12:52.194306: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (h2.carbonate.uits.iu.edu): /proc/driver/nvidia/version does not exist\n",
      "2023-10-16 11:12:52.203295: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from vqvae3d_monai import VQVAE\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "from dipy.io.image import load_nifti\n",
    "from dipy.align.reslice import reslice\n",
    "from scipy.ndimage import affine_transform\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "num_gpus = strategy.num_replicas_in_sync\n",
    "print(f'Number of devices: {num_gpus}')\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "print(tf.config.list_logical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf69ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images in dataset:  484\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_list(dataset):\n",
    "    dataset_list = []\n",
    "    \n",
    "    if dataset == 'CC':\n",
    "        files = ['./dataset_cc359.txt']\n",
    "    elif dataset == 'NFBS':\n",
    "        files = ['./dataset.txt']\n",
    "    elif dataset == 'both':\n",
    "        files = ['./dataset.txt']\n",
    "        files.append('./dataset_cc359.txt')\n",
    "\n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            lin = f.readline()\n",
    "            while(lin):\n",
    "                dataset_list.append(lin[:-1])\n",
    "                lin = f.readline()\n",
    "    print('Total Images in dataset: ', len(dataset_list))\n",
    "    return dataset_list\n",
    "\n",
    "def datasetHelperFunc(path):\n",
    "    transform_vol, mask = None, None\n",
    "    if isinstance(path, bytes):\n",
    "        path = str(path.decode('utf-8'))\n",
    "\n",
    "    if 'CC' in path:\n",
    "        vol, affine, voxsize = load_nifti(\n",
    "            '/N/project/grg_data/data/skullstripping_datasets/CC359/Original/'+path, return_voxsize=True)\n",
    "        mask, _ = load_nifti(\n",
    "            '/N/project/grg_data/data/skullstripping_datasets/CC359/STAPLE/'+path[:-7]+'_staple.nii.gz')\n",
    "        mask[mask < 1] = 0  # Values <1 in the mask is background\n",
    "        vol = vol*mask\n",
    "    else:\n",
    "        #vol, affine, voxsize = load_nifti(str(path.decode('utf-8')), return_voxsize=True)\n",
    "        vol, affine, voxsize = load_nifti(path, return_voxsize=True)\n",
    "        mask, _ = load_nifti(path[:-7]+'mask.nii.gz')\n",
    "        mask[mask < 1] = 0  # Values <1 in the mask is background\n",
    "        vol = vol*mask\n",
    "\n",
    "    if mask is not None:\n",
    "        mask, _ = transform_img(mask, affine, voxsize)\n",
    "        # Handling negative pixels, occurred as a result of preprocessing\n",
    "        mask[mask < 0] *= -1\n",
    "        mask = np.expand_dims(mask, -1)\n",
    "    transform_vol, _ = transform_img(vol, affine, voxsize)\n",
    "    # Handling negative pixels, occurred as a result of preprocessing\n",
    "    transform_vol[transform_vol < 0] *= -1\n",
    "    transform_vol = (transform_vol-np.min(transform_vol)) / \\\n",
    "        (np.max(transform_vol)-np.min(transform_vol))\n",
    "    transform_vol = np.expand_dims(transform_vol, -1)\n",
    "    return tf.convert_to_tensor(transform_vol, tf.float32), tf.convert_to_tensor(mask, tf.float32)\n",
    "\n",
    "\n",
    "dataset_list = get_dataset_list('both')\n",
    "\n",
    "bs = 48\n",
    "suffix = 'B48-both'\n",
    "test_size = len(dataset_list) - (len(dataset_list)//bs)*bs\n",
    "\n",
    "lis = dataset_list[-test_size:]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(lis)\n",
    "\n",
    "dataset = dataset.map(lambda x: tf.numpy_function(func=datasetHelperFunc, inp=[x], Tout=[tf.float32, tf.float32]),\n",
    "                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = dataset.batch(bs).prefetch(tf.data.experimental.AUTOTUNE).take(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e3cf2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2b34ebe2a740>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VQVAE(\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            num_channels=(32, 64, 128),\n",
    "            num_res_channels=(32, 64, 128),\n",
    "            num_res_layers=3,\n",
    "            downsample_parameters=((2, 4, 1, 'same'), (2, 4, 1, 'same'), (2, 4, 1, 'same')),\n",
    "            upsample_parameters=((2, 4, 1, 'same', 0), (2, 4, 1, 'same', 0), (2, 4, 1, 'same')),\n",
    "            num_embeddings=256,\n",
    "            embedding_dim=32,\n",
    "            num_gpus=float(num_gpus),\n",
    "            kernel_resize=False)\n",
    "\n",
    "test_epoch = 64\n",
    "model.load_weights(os.path.join('./checkpoints-vqvae-monai-scaled-128', suffix, str(test_epoch)+'.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f410a84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 11:12:55.222790: W tensorflow/core/framework/op_kernel.cc:1768] RESOURCE_EXHAUSTED: MemoryError: \n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/tmp/ipykernel_6873/3510424823.py\", line 27, in datasetHelperFunc\n",
      "    vol, affine, voxsize = load_nifti(\n",
      "\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/dipy/io/image.py\", line 67, in load_nifti\n",
      "    data = np.asanyarray(img.dataobj) if as_ndarray else img.dataobj\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/arrayproxy.py\", line 391, in __array__\n",
      "    arr = self._get_scaled(dtype=dtype, slicer=())\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/arrayproxy.py\", line 358, in _get_scaled\n",
      "    scaled = apply_read_scaling(self._get_unscaled(slicer=slicer), scl_slope, scl_inter)\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/arrayproxy.py\", line 332, in _get_unscaled\n",
      "    return array_from_file(self._shape,\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/volumeutils.py\", line 522, in array_from_file\n",
      "    n_read = infile.readinto(data_bytes)\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/gzip.py\", line 301, in read\n",
      "    return self._buffer.read(size)\n",
      "\n",
      "MemoryError\n",
      "\n",
      "\n",
      "2023-10-16 11:12:55.293225: W tensorflow/core/framework/op_kernel.cc:1768] RESOURCE_EXHAUSTED: MemoryError: \n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/tmp/ipykernel_6873/3510424823.py\", line 27, in datasetHelperFunc\n",
      "    vol, affine, voxsize = load_nifti(\n",
      "\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/dipy/io/image.py\", line 67, in load_nifti\n",
      "    data = np.asanyarray(img.dataobj) if as_ndarray else img.dataobj\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/arrayproxy.py\", line 391, in __array__\n",
      "    arr = self._get_scaled(dtype=dtype, slicer=())\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/arrayproxy.py\", line 358, in _get_scaled\n",
      "    scaled = apply_read_scaling(self._get_unscaled(slicer=slicer), scl_slope, scl_inter)\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/arrayproxy.py\", line 332, in _get_unscaled\n",
      "    return array_from_file(self._shape,\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/volumeutils.py\", line 522, in array_from_file\n",
      "    n_read = infile.readinto(data_bytes)\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/gzip.py\", line 301, in read\n",
      "    return self._buffer.read(size)\n",
      "\n",
      "MemoryError\n",
      "\n",
      "\n",
      "2023-10-16 11:12:55.322297: W tensorflow/core/framework/op_kernel.cc:1768] RESOURCE_EXHAUSTED: MemoryError: \n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/tmp/ipykernel_6873/3510424823.py\", line 27, in datasetHelperFunc\n",
      "    vol, affine, voxsize = load_nifti(\n",
      "\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/dipy/io/image.py\", line 67, in load_nifti\n",
      "    data = np.asanyarray(img.dataobj) if as_ndarray else img.dataobj\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/arrayproxy.py\", line 391, in __array__\n",
      "    arr = self._get_scaled(dtype=dtype, slicer=())\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/arrayproxy.py\", line 358, in _get_scaled\n",
      "    scaled = apply_read_scaling(self._get_unscaled(slicer=slicer), scl_slope, scl_inter)\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/arrayproxy.py\", line 332, in _get_unscaled\n",
      "    return array_from_file(self._shape,\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/volumeutils.py\", line 522, in array_from_file\n",
      "    n_read = infile.readinto(data_bytes)\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/gzip.py\", line 301, in read\n",
      "    return self._buffer.read(size)\n",
      "\n",
      "MemoryError\n",
      "\n",
      "\n",
      "2023-10-16 11:12:55.396080: W tensorflow/core/framework/op_kernel.cc:1768] RESOURCE_EXHAUSTED: MemoryError: \n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/tmp/ipykernel_6873/3510424823.py\", line 27, in datasetHelperFunc\n",
      "    vol, affine, voxsize = load_nifti(\n",
      "\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/dipy/io/image.py\", line 67, in load_nifti\n",
      "    data = np.asanyarray(img.dataobj) if as_ndarray else img.dataobj\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/arrayproxy.py\", line 391, in __array__\n",
      "    arr = self._get_scaled(dtype=dtype, slicer=())\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/arrayproxy.py\", line 358, in _get_scaled\n",
      "    scaled = apply_read_scaling(self._get_unscaled(slicer=slicer), scl_slope, scl_inter)\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/arrayproxy.py\", line 332, in _get_unscaled\n",
      "    return array_from_file(self._shape,\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/volumeutils.py\", line 522, in array_from_file\n",
      "    n_read = infile.readinto(data_bytes)\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/gzip.py\", line 301, in read\n",
      "    return self._buffer.read(size)\n",
      "\n",
      "MemoryError\n",
      "\n",
      "\n",
      "Exception ignored in: <generator object _tokenize at 0x80519e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/asttokens/asttokens.py\", line 85, in _generate_tokens\n",
      "MemoryError: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "  File \"/tmp/ipykernel_6873/631462050.py\", line 1, in <cell line: 1>\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 766, in __next__\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 749, in _next_internal\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 3017, in iterator_get_next\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 7209, in raise_from_not_ok_status\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} MemoryError: \n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/tmp/ipykernel_6873/3510424823.py\", line 27, in datasetHelperFunc\n",
      "    vol, affine, voxsize = load_nifti(\n",
      "\n",
      "  File \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/dipy/io/image.py\", line 67, in load_nifti\n",
      "    data = np.asanyarray(img.dataobj) if as_ndarray else img.dataobj\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/arrayproxy.py\", line 391, in __array__\n",
      "    arr = self._get_scaled(dtype=dtype, slicer=())\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/arrayproxy.py\", line 358, in _get_scaled\n",
      "    scaled = apply_read_scaling(self._get_unscaled(slicer=slicer), scl_slope, scl_inter)\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/arrayproxy.py\", line 332, in _get_unscaled\n",
      "    return array_from_file(self._shape,\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/nibabel/volumeutils.py\", line 522, in array_from_file\n",
      "    n_read = infile.readinto(data_bytes)\n",
      "\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/gzip.py\", line 301, in read\n",
      "    return self._buffer.read(size)\n",
      "\n",
      "MemoryError\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]] [Op:IteratorGetNext]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/executing/executing.py\", line 317, in executing\n",
      "KeyError: (<code object raise_from_not_ok_status at 0x2b3425cbf730, file \"/N/u/aajais/Carbonate/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 7207>, 47502972417840, 40)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 799, in format_exception_as_a_whole\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 854, in get_records\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/stack_data/core.py\", line 565, in stack_data\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/stack_data/utils.py\", line 84, in collapse_repeated\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/stack_data/core.py\", line 555, in mapper\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/stack_data/core.py\", line 520, in __init__\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/executing/executing.py\", line 369, in executing\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/executing/executing.py\", line 252, in for_frame\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/executing/executing.py\", line 270, in for_filename\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/executing/executing.py\", line 281, in _for_filename_and_lines\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/stack_data/core.py\", line 81, in __init__\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/executing/executing.py\", line 413, in asttokens\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/asttokens/asttokens.py\", line 59, in __init__\n",
      "  File \"/N/soft/rhel7/deeplearning/Python-3.10.5/lib/python3.10/site-packages/asttokens/asttokens.py\", line 85, in _generate_tokens\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "for x, _ in test_dataset:\n",
    "    np.save(f'./reconst_scaled_vqvae3d_monai/original-{suffix}.npy', x.numpy())\n",
    "    reconst = model(x)\n",
    "    loss = tf.reduce_mean((reconst-x)**2)\n",
    "    print(f'Test Loss is {loss}')\n",
    "    np.save(f'./reconst_scaled_vqvae3d_monai/reconst3d-{suffix}-epoch{test_epoch}.npy', reconst.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a68a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, _ in test_dataset:\n",
    "    print(x.numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1eac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vqvae3d_monai import VQVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad7a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
